region = "${?AWS_REGION}"
access-key = "${?AWS_ACCESS_KEY_ID}"
secret-key = "${?AWS_SECRET_ACCESS_KEY}"
# Optional: Override endpoint for S3
s3-endpoint = "${?MINIO_ENDPOINT}"
connection-ssl-enabled = "${?S3_SSL_ENABLED}"

environment = "development"
module {
  name = "ingestion"
  description = "Data Ingestion Module"
  version = "1.0.0"
}

spark {
  master = "local[*]"
  app-name = "${app.name}-${module.name}-${environment}"

  executor {
    memory = "2g"
    cores = 3
    instances = 1
  }

  driver {
    memory = "1g"
    max-result-size = "1g"
  }

  dynamic-allocation {
    enabled = false
    min-executors = 1
    max-executors = 4
    initial-executors = 2
  }

  sql {
    shuffle-partitions = 8
    adaptive {
      enabled = false
      advisory-partition-size = "64MB"
    }
  }

  ui {
    enabled = true
    port = 4040
    retain-tasks = 1000
    retain-stages = 1000
  }

  eventLog {
    enabled = true
    dir = "/tmp/spark-events-dev"
    compress = false
  }

  # Development-specific settings
  debug {
    max-to-string-fields = 100
  }

  jars {
    packages = "org.apache.hadoop:hadoop-aws:3.4.1,org.postgresql:postgresql:42.7.7,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0"
    lib = []
  }

  # Hadoop Configuration for Local S3/MinIO
  hadoop {
    fs {
      s3a {
        path.style.access = "true"
        aws.credentials.provider = "software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider"
        fast.upload.buffer = "disk"
        impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
        committer.name = "directory"
        # AWS credentials for local development
        region = region
        access.key = "minioadmin"
        secret.key = "minioadmin"
        # Optional: Override endpoint for MinIO
        endpoint = "http://127.0.0.1:9000/"
        connection.ssl.enabled = false
      }
    }
    mapreduce {
      fileoutputcommitter {
        cleanup-failures.ignored = "true"
      }
    }
  }
}

kafka {
  bootstrap-servers = "127.0.0.1:9094"
  group-id = "${spark-app-name}-${environment}"

  consumer {
    max-poll-records = 500
    group-instance-id = ${?HOSTNAME}
  }

  producer {
    batch-size = 16384
    linger-ms = 1
  }

  topics = "sensors.temperature,sensors.motion,sensors.air_quality"
  checkpoint-location = ${storage.datalake.base-url}${environment}"/bronze/_checkpoints"
}

storage {
  datalake {
    s3-endpoint = "http://localhost:9000"
    base-url = "s3a://smartstar/"
    output = ${storage.datalake.base-url}${environment}"/bronze/sensors"
    checkpoint-output = ${storage.datalake.base-url}${environment}"/bronze/_checkpoints"
  }

  # Development-specific paths
  paths {
    logs = "/tmp/smartstar/dev/logs"
    debug = "/tmp/smartstar/dev/debug"
  }
}