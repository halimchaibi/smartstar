region = "${?AWS_REGION}"
access-key = "${?AWS_ACCESS_KEY_ID}"
secret-key = "${?AWS_SECRET_ACCESS_KEY}"
# Optional: Override endpoint for S3
s3-endpoint = "${?MINIO_ENDPOINT}"
connection-ssl-enabled = "${?S3_SSL_ENABLED}"

environment = "development"
module {
  name = "ingestion"
  description = "Data Ingestion Module"
  version = "1.0.0"
}

spark {
  master = "local[*]"
  app-name = "${app.name}-${module.name}-${environment}"

  executor {
    memory = "2g"
    cores = 3
    instances = 1
  }

  driver {
    memory = "1g"
    max-result-size = "1g"
  }

  dynamic-allocation {
    enabled = false
    min-executors = 1
    max-executors = 4
    initial-executors = 2
  }

  sql {
    shuffle-partitions = 8
    adaptive {
      enabled = false
      advisory-partition-size = "64MB"
    }
  }

  ui {
    enabled = true
    port = 4040
    retain-tasks = 1000
    retain-stages = 1000
  }

  eventLog {
    enabled = false
  }

  # Development-specific settings
  debug {
    max-to-string-fields = 100
  }

  jars {
    packages = "org.apache.hadoop:hadoop-aws:3.4.1,org.postgresql:postgresql:42.7.7,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,org.apache.iceberg:iceberg-core:1.10.0"
  }

  # Hadoop Configuration for Local S3/MinIO
  hadoop {
    fs {
      s3a {
        path.style.access = "true"
        aws.credentials.provider = "software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider"
        fast.upload.buffer = "disk"
        impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
        committer.name = "directory"
        # AWS credentials for local development
        region = region
        access.key = "minioadmin"
        secret.key = "minioadmin"
        # Override endpoint for MinIO (default for Docker, override with MINIO_ENDPOINT for local)
        endpoint = "http://smartstar-minio:9000/"
        endpoint = ${?MINIO_ENDPOINT}
        connection.ssl.enabled = false
      }
    }
    mapreduce {
      fileoutputcommitter {
        cleanup-failures.ignored = "true"
      }
    }
    # Windows-specific settings
    io {
      native.lib.available = "false"
    }
    tmp.dir = "C:/tmp/hadoop"
  }
}

kafka {
  # Default for local dev, can be overridden by KAFKA_BOOTSTRAP_SERVERS env var
  bootstrap-servers = "smartstar-kafka:9092"
  bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}
  group-id = "${spark-app-name}-${environment}"

  consumer {
    max-poll-records = 500
    group-instance-id = ${?HOSTNAME}
  }

  producer {
    batch-size = 16384
    linger-ms = 1
  }

  topics = "sensors.temperature,sensors.motion,sensors.air_quality"
  checkpoint-location = ${storage.datalake.base-url}${environment}"/bronze/_checkpoints"
  starting-offsets = "earliest"
}

storage {
  datalake {
    s3-endpoint = "http://smartstar-minio:9000"
    s3-endpoint = ${?MINIO_ENDPOINT}
    base-url = "s3a://smartstar/"
    output = ${storage.datalake.base-url}${environment}"/bronze/sensors"
    checkpoint-output = ${storage.datalake.base-url}${environment}"/bronze/_checkpoints"
  }

  # Development-specific paths
  paths {
    logs = "/tmp/smartstar/dev/logs"
    debug = "/tmp/smartstar/dev/debug"
  }
}