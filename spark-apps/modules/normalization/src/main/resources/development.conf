region = "${?AWS_REGION}"
access-key = "${?AWS_ACCESS_KEY_ID}"
secret-key = "${?AWS_SECRET_ACCESS_KEY}"
# Optional: Override endpoint for S3
s3-endpoint = "${?MINIO_ENDPOINT}"
connection-ssl-enabled = "${?S3_SSL_ENABLED}"

environment = "development"
module {
  name = "normalization"
  description = "Data normalization Module"
  version = "1.0.0"
}

spark {
  master = "local[*]"
  app-name = "${app.name}-${module.name}-${environment}"

  executor {
    memory = "2g"
    cores = 3
    instances = 1
  }

  driver {
    memory = "1g"
    max-result-size = "1g"
  }

  dynamic-allocation {
    enabled = false
    min-executors = 1
    max-executors = 4
    initial-executors = 2
  }

  sql {
    shuffle-partitions = 8
    adaptive {
      enabled = false
      advisory-partition-size = "64MB"
    }
    catalog {
      sensors {
        type = "jdbc"
        uri = "jdbc:postgresql://localhost:5432/smartstar"
        jdbc {
          user = "smartstar"
          password = "smartstar"
        }
        warehouse = "s3a://smartstar/development/silver/sensors/"
      }
      #spark_catalog = "sensors"
    }
    extensions = "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
  }

  ui {
    enabled = true
    port = 4040
    retain-tasks = 1000
    retain-stages = 1000
  }

  eventLog {
    enabled = false
  }

  # Development-specific settings
  debug {
    max-to-string-fields = 100
  }

  jars {
    packages = "org.apache.hadoop:hadoop-aws:3.4.1,org.postgresql:postgresql:42.7.7,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,org.apache.iceberg:iceberg-core:1.10.0"
  }

  # Hadoop Configuration for Local S3/MinIO
  hadoop {
    fs {
      s3a {
        path.style.access = "true"
        aws.credentials.provider = "software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider"
        fast.upload.buffer = "array"
        impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
        committer.name = "directory"
        # AWS credentials for local development
        region = region
        access.key = "minioadmin"
        secret.key = "minioadmin"
        # Optional: Override endpoint for MinIO
        endpoint = "http://127.0.0.1:9000/"
        connection.ssl.enabled = false
      }
    }
    mapreduce {
      fileoutputcommitter {
        cleanup-failures.ignored = "true"
      }
    }
    # Windows-specific settings
    io {
      native.lib.available = "false"
    }
    tmp.dir = "C:/tmp/hadoop"
  }
}

storage {
  datalake {
    s3-endpoint = "http://localhost:9000"
    base-url = "s3a://smartstar/"
    output = ${storage.datalake.base-url}${environment}"/silver/sensors/temperature"
    checkpoint-output = ${storage.datalake.base-url}${environment}"/silver/_checkpoints"
  }

  # Development-specific paths
  paths {
    logs = "/tmp/smartstar/dev/logs"
    debug = "/tmp/smartstar/dev/debug"
  }
}

streaming {
  checkpoint-location = ${storage.datalake.base-url}${environment}"/silver/_checkpoints"
  input = ${storage.datalake.base-url}${environment}"/bronze/sensors/"
}