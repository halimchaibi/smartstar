
module {
  name = "analytics"
  description = "Analytics Module"
  version = "1.0.0"
}

# Analytics-specific Spark settings
spark {
  # Analytics typically needs ML libraries
  jars {
    packages = [
      "org.apache.spark:spark-mllib_2.13:3.5.0"
    ]
  }

  sql {
    # Analytics workloads often need more partitions for complex queries
    shuffle-partitions = 800

    adaptive {
      # Optimize for analytical queries
      advisory-partition-size = "1GB"
      non-empty-partition-ratio-for-broadcast-join = 0.05
    }

    # Enable statistics collection for analytics
    statistics {
      auto-size-update = true
      cbo-enabled = true
    }
  }

  # ML specific settings
  mllib {
    # Use BLAS for better performance
    blas = "netlib-java"

    # Increase driver memory for ML models
    driver-max-result-size = "8g"
  }
}

# Analytics-specific storage paths
storage {
  paths {
    # Analytics-specific paths
    models = ${storage.base-path}"/analytics/models"
    features = ${storage.base-path}"/analytics/features"
    metrics = ${storage.base-path}"/analytics/metrics"
    reports = ${storage.base-path}"/analytics/reports"
    experiments = ${storage.base-path}"/analytics/experiments"
  }
}

# Machine learning configuration
ml {
  # Feature store settings
  feature-store {
    enabled = true
    backend = "delta"
    versioning = true
  }

  # Model registry settings
  model-registry {
    enabled = true
    backend = "mlflow"
    tracking-uri = ${?MLFLOW_TRACKING_URI}
  }

  # Experiment tracking
  experiments {
    auto-log = true
    log-models = true
    log-artifacts = true
  }
}

# Analytics-specific data quality (more focused on statistical validation)
data-quality {
  rules {
    statistical-validation = true
    outlier-detection = true
    correlation-analysis = true
    distribution-analysis = true
  }

  thresholds {
    statistical-significance = 0.95
    outlier-threshold = 3.0  # 3 standard deviations
  }
}
