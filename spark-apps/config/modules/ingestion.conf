# Module identification
module {
  name = "ingestion"
  description = "Data Ingestion Module"
  version = "1.0.0"
}

# Ingestion-specific Spark settings
spark {
  # Ingestion typically needs more shuffle partitions for initial data processing
  sql {
    shuffle-partitions = ${?INGESTION_SHUFFLE_PARTITIONS}
    adaptive {
      # More aggressive coalescing for ingestion workloads
      advisory-partition-size = "256MB"
    }
  }

  # Streaming specific settings for ingestion
  streaming {
    checkpoint-location = ${storage.paths.checkpoints}"/ingestion"

    # Ingestion-specific streaming settings
    trigger {
      processing-time = "30s"
      once = false
    }

    # State management
    state-store {
      provider-class = "org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider"
    }
  }
}

# Ingestion-specific Kafka settings
kafka {
  # Ingestion typically uses larger consumer groups
  consumer {
    max-poll-records = 5000
    fetch-min-bytes = 65536
    auto-offset-reset = "earliest"
  }

  # Topic configuration for ingestion
  topics {
    raw-data = "smartstar-raw-data"
    processed-data = "smartstar-processed-data"
    errors = "smartstar-ingestion-errors"
    dead-letter = "smartstar-ingestion-dlq"
  }
}

# Ingestion-specific storage paths
storage {
  paths {
    # Override common paths for ingestion-specific structure
    raw = ${storage.base-path}"/ingestion/raw"
    validated = ${storage.base-path}"/ingestion/validated"
    failed = ${storage.base-path}"/ingestion/failed"
    quarantine = ${storage.base-path}"/ingestion/quarantine"
  }
}

# Ingestion-specific data quality rules
data-quality {
  # Stricter validation for ingestion
  fail-on-error = true

  rules {
    schema-validation = true
    duplicate-detection = true
    data-profiling = true
  }

  thresholds {
    error-rate = 0.02  # Stricter for ingestion
    schema-compliance = 0.99
  }
}

# File format specific settings for ingestion
formats {
  csv {
    header = true
    delimiter = ","
    quote = "\""
    escape = "\\"
    null-value = ""
    timestamp-format = "yyyy-MM-dd HH:mm:ss"
    date-format = "yyyy-MM-dd"
  }

  json {
    multiline = false
    allow-comments = false
    allow-unquoted-field-names = false
    allow-single-quotes = false
    allow-numeric-leading-zeros = false
  }

  avro {
    compression = "snappy"
    use-avro-native-reader = true
  }
}