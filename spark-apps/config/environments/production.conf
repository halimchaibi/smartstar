
include "../common.conf"

environment = "production"

spark {
  master = "yarn"

  executor {
    memory = "8g"
    cores = 5
    instances = 50
  }

  driver {
    memory = "4g"
    max-result-size = "4g"
  }

  dynamic-allocation {
    enabled = true
    min-executors = 5
    max-executors = 100
    initial-executors = 20
    target-executors = 50
    executor-idle-timeout = "60s"
    cached-executor-idle-timeout = "300s"
    scheduler-backlog-timeout = "1s"
    sustained-scheduler-backlog-timeout = "1s"
  }

  sql {
    shuffle-partitions = 400
    adaptive {
      advisory-partition-size = "512MB"
      non-empty-partition-ratio-for-broadcast-join = 0.1
      max-shuffled-hash-join-local-map-threshold = "256MB"
    }
  }

  ui {
    enabled = false  # Security: disabled in production
  }

  eventLog {
    enabled = true
    dir = "hdfs://namenode:8020/spark-events-prod"
    compress = true
    rolling {
      enabled = true
      max-file-size = "128m"
    }
  }

  # Production optimizations
  storage {
    level = "MEMORY_AND_DISK_SER_2"  # Replicated for fault tolerance
    fraction = 0.8
  }

  # Hadoop/S3 optimizations
  hadoop {
    fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
    fs.s3a.aws.credentials.provider = "org.apache.hadoop.fs.s3a.InstanceProfileCredentialsProvider"
    fs.s3a.connection.maximum = 200
    fs.s3a.threads.max = 64
    fs.s3a.connection.establish.timeout = 5000
    fs.s3a.connection.timeout = 200000
    fs.s3a.fast.upload = true
    fs.s3a.multipart.size = "104857600"  # 100MB
    fs.s3a.multipart.threshold = "209715200"  # 200MB
  }

  streaming {
    stop-gracefully-on-shutdown = true
    checkpoint-duration-multiplier = 10
  }
}

database {
  host = "prod-postgres-cluster.internal"
  port = 5432
  name = "smartstar_prod"
  username = ${SMARTSTAR_DB_USER}
  password = ${SMARTSTAR_DB_PASSWORD}
  ssl = true
  connection-pool-size = 50
  max-lifetime = "30m"
  connection-timeout = "30s"
  validation-timeout = "5s"
}

kafka {
  bootstrap-servers = "kafka-prod-1:9092,kafka-prod-2:9092,kafka-prod-3:9092"
  group-id = ${kafka.group-id-prefix}"-prod"

  consumer {
    max-poll-records = 2000
    fetch-min-bytes = 100000
    max-partition-fetch-bytes = 2097152
    session-timeout = "30s"
    heartbeat-interval = "3s"
  }

  producer {
    batch-size = 131072
    linger-ms = 10
    compression-type = "lz4"
    max-request-size = 2097152
    acks = "all"
    retries = 3
    max-in-flight-requests-per-connection = 1
  }
}

storage {
  base-path = "s3a://smartstar-prod-bucket/data"
}

monitoring {
  logging {
    level = "WARN"

    loggers {
      "com.smartstar" = "INFO"
      "org.apache.spark" = "ERROR"
      "org.apache.hadoop" = "ERROR"
    }
  }

  # Production monitoring
  alerting {
    enabled = true
    error-threshold = 0.01
    latency-threshold = "5m"
  }
}
