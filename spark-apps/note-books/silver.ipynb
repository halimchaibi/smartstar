{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26643db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homelab/dev/bronze/sensors/topic=sensors.temperature/year=2025/month=8/day=26\n",
    "s3_endpoint = \"http://192.168.1.196:32000\"\n",
    "env = \"dev\"\n",
    "bucket = \"homelab\"\n",
    "\n",
    "input_bronze_prefix = \"bronze/sensors/topic=sensors.temperature\"\n",
    "output_silver_prefix = \"silver/sensors\"\n",
    "\n",
    "topic = \"sensors.temperature\"\n",
    "\n",
    "s3_topic_path = f\"s3a://{bucket}/{env}/{input_bronze_prefix}\"\n",
    "checkpoint_dir = f\"s3a://{bucket}/{env}/{output_silver_prefix}/_checkpoints/sensors/temperature\"\n",
    "warehouse =  f\"s3a://{bucket}/{env}/{output_silver_prefix}\"\n",
    "\n",
    "logLevel = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05351f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_topic_path)\n",
    "print(checkpoint_dir)\n",
    "print(warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ce35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, to_date\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"BronzeExplorationS3\")\n",
    "    .master(\"local[*]\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # S3 configuration\n",
    "    # -----------------------------\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"fs.s3a.endpoint\", s3_endpoint)\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"vYWL9V9xFeIIll7mTmUX\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"rAni4j7zN1aKgoWezNBQfFFfIXLZBgpis6AB7CUV\")\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")  # For HTTP endpoints\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    # -----------------------------\n",
    "    # Required jars\n",
    "    # -----------------------------\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.1,\"\n",
    "        \"org.postgresql:postgresql:42.7.7\"\n",
    ")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/home/hchaibi/projects/homelab/smartstar/note-books/\"\n",
    "        \"iceberg-spark-runtime-4.0_2.13-1.10.0-20250822.002003-111.jar\",\n",
    "    )\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Iceberg JDBC catalog\n",
    "    # -----------------------------\n",
    "    .config(\"spark.sql.catalog.sensors\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.sensors.type\", \"jdbc\")\n",
    "    .config(\"spark.sql.catalog.sensors.uri\", \"jdbc:postgresql://localhost:5432/iceberg\")\n",
    "    .config(\"spark.sql.catalog.sensors.jdbc.user\", \"iceberg\")\n",
    "    .config(\"spark.sql.catalog.sensors.jdbc.password\", \"iceberg\")\n",
    "    .config(\"spark.sql.catalog.sensors.warehouse\", warehouse)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Iceberg Spark extensions\n",
    "    # -----------------------------\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.defaultCatalog\", \"sensors\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(logLevel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_schema = StructType(\n",
    "    [\n",
    "        StructField(\"event_time\", StringType(), True),\n",
    "        StructField(\"ingestion_ts\", StringType(), True),\n",
    "        StructField(\"kafka_timestamp\", StringType(), True),\n",
    "        StructField(\"offset\", LongType(), True),\n",
    "        StructField(\"partition\", LongType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "payload_schema = StructType(\n",
    "    [\n",
    "        StructField(\"device_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),  # cast later to TimestampType\n",
    "        StructField(\"sensor_type\", StringType(), True),\n",
    "        StructField(\n",
    "            \"location\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"latitude\", DoubleType(), True),\n",
    "                    StructField(\"longitude\", DoubleType(), True),\n",
    "                    StructField(\"city\", StringType(), True),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\"temperature\", DoubleType(), True),\n",
    "        StructField(\"humidity\", DoubleType(), True),\n",
    "        StructField(\"unit\", StringType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Iceberg table exists (create if missing)\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS sensors.temperature\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sensors.temperature (\n",
    "  device_id STRING,\n",
    "  device_ts TIMESTAMP,\n",
    "  sensor_type STRING,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  city STRING,\n",
    "  temperature DOUBLE,\n",
    "  humidity DOUBLE,\n",
    "  unit STRING,\n",
    "  event_time TIMESTAMP,\n",
    "  year INT,\n",
    "  month INT,\n",
    "  day INT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (year, month, day)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Iceberg catalog and table bootstrap complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b129733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream JSON from the s3 topic path\n",
    "df_raw = (\n",
    "    spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(raw_schema)\n",
    "    .option(\"maxFilesPerTrigger\", 10)  # Reduced for easier debugging\n",
    "    .option(\"pathGlobFilter\", \"*.json\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")  # This allows malformed records\n",
    "    .load(s3_topic_path)\n",
    ")\n",
    "\n",
    "df_decoded = (\n",
    "    df_raw.withColumn(\"data\", from_json(col(\"value\"), payload_schema))\n",
    "          .select(                      # flatten the structure\n",
    "              \"partition\",\n",
    "              \"offset\",\n",
    "              \"timestamp\",\n",
    "              \"event_time\",\n",
    "              col(\"data.device_id\"),\n",
    "              col(\"data.timestamp\").alias(\"device_ts\"),\n",
    "              col(\"data.sensor_type\"),\n",
    "              col(\"data.location.latitude\"),\n",
    "              col(\"data.location.longitude\"),\n",
    "              col(\"data.location.city\"),\n",
    "              col(\"data.temperature\"),\n",
    "              col(\"data.humidity\"),\n",
    "              col(\"data.unit\"),\n",
    "              col(\"year\"),\n",
    "              col(\"month\"),\n",
    "              col(\"day\")\n",
    "          )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_decoded\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        \"device_ts\",\n",
    "        \"sensor_type\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"city\",\n",
    "        \"temperature\",\n",
    "        \"humidity\",\n",
    "        \"unit\",\n",
    "        \"event_time\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"device_ts\", to_timestamp(col(\"device_ts\"))\n",
    "    )\n",
    "        .withColumn(\n",
    "        \"event_time\", to_timestamp(col(\"event_time\"))\n",
    "    )\n",
    "    .writeStream.format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_dir)\n",
    "    .toTable(\"sensors.temperature\")   # ✅ correct for Iceberg\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830db50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Write the decoded stream to console ---\n",
    "query = (\n",
    "    df_decoded.writeStream\n",
    "              .format(\"console\")\n",
    "              .outputMode(\"append\")        # \"append\" is usually what you want for streams\n",
    "              .option(\"truncate\", False)\n",
    "              .start()\n",
    ")\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24212336",
   "metadata": {},
   "source": [
    "#### Get distinct topics from the raw dataframe\n",
    "```python\n",
    "topics = [row.topic for row in df_raw.select(\"topic\").distinct().collect()]\n",
    "\n",
    "\n",
    "schemas = {}\n",
    "for t in topics:\n",
    "    sample_path = f\"s3a://homelab/dev/bronze/sensors/topic={t}/\"\n",
    "    inferred = spark.read.json(sample_path).schema\n",
    "    schemas[t] = inferred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988c966",
   "metadata": {},
   "source": [
    "```python\n",
    "for topic_name, schema in schemas.items():\n",
    "    df_topic = (\n",
    "        df_raw.filter(col(\"topic\") == topic_name)\n",
    "           .withColumn(\"data\", from_json(col(\"value\"), schema))\n",
    "           .select(\"topic\", \"year\", \"month\", \"day\", \"data.*\")\n",
    "    )\n",
    "\n",
    "    # cleansing: e.g. drop nulls\n",
    "    df_topic_clean = df_topic.na.drop(subset=[\"device_id\", \"timestamp\"])\n",
    "\n",
    "    # enrichment: e.g. add partition_date column\n",
    "    df_topic_enriched = df_topic_clean.withColumn(\n",
    "        \"partition_date\", to_date(col(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    # write to Silver layer in parquet/Delta\n",
    "    (\n",
    "        df_topic_enriched.writeStream\n",
    "            .format(\"parquet\")\n",
    "            .option(\"numRows\", 20)\n",
    "            .option(\"path\", f\"s3a://homelab/dev/silver/{topic_name}/\")\n",
    "            .option(\"checkpointLocation\", f\"s3a://homelab/checkpoints/{topic_name}/\")\n",
    "            .partitionBy(\"year\", \"month\", \"day\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(processingTime='10 seconds')\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b6f1f",
   "metadata": {},
   "source": [
    "```python\n",
    "sample = (\n",
    "    spark.read\n",
    "         .format(\"json\")\n",
    "         .option(\"pathGlobFilter\", \"*.json\")\n",
    "         .load(\"s3a://homelab/dev/bronze/sensors/\")\n",
    ")\n",
    "\n",
    "sample.printSchema()\n",
    "schema = sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6b47d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02af8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Nessie as Iceberg catalog\n",
    "spark.sql.catalog.nessie=org.apache.iceberg.nessie.NessieCatalog\n",
    "spark.sql.catalog.nessie.uri=http://<nessie-server>:19120/api/v1\n",
    "spark.sql.catalog.nessie.ref=main\n",
    "spark.sql.catalog.nessie.auth.type=NONE       # or BEARER if auth enabled\n",
    "spark.sql.catalog.nessie.warehouse=s3a://homelab/dev/silver/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6455e",
   "metadata": {},
   "source": [
    "```python\n",
    "# Start a local Spark session for exploration\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BronzeExplorationS3\")\n",
    "    .master(\"local[*]\")\n",
    "    # needed for s3a:// access\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"fs.s3a.endpoint\", s3_endpoint)\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"vYWL9V9xFeIIll7mTmUX\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.secret.key\", \"rAni4j7zN1aKgoWezNBQfFFfIXLZBgpis6AB7CUV\"\n",
    "    )\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\")\n",
    "    # Required packages for S3 and Iceberg\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/home/hchaibi/projects/homelab/smartstar/note-books/iceberg-spark-runtime-4.0_2.13-1.10.0-20250822.002003-111.jar\",\n",
    "    )\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.1\")\n",
    "    # Iceberg catalog config (example for S3)\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(logLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2732b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
